{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BIGmind\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import NLP_model\n",
    "import utl\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare test input data, train data, and evaluate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "# Use different data as test data\n",
    "ref_data=pd.read_excel('./data/s2f_v2.xlsx').iloc[:,:].values #用于人工结果检查的参考数据\n",
    "ref_data2=pd.read_excel('./data/Book2.xlsx').iloc[:,:].values\n",
    "F_data = ref_data2.copy()[:,0].tolist()\n",
    "S_data = ref_data2.copy()[:,1].tolist()\n",
    "\n",
    "# Use different data as evaluate data\n",
    "Feature_eval = ref_data2.copy()[:1000,0].tolist()\n",
    "Scenario_eval = ref_data2.copy()[:1000,1].tolist()\n",
    "\n",
    "# Train data\n",
    "Feature_list = ref_data[:,0].tolist()\n",
    "Scenario_list = ref_data[:,1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本检验与清洗\n",
    "dic = ['~','!','@','#','$','%','^','&','*','(',')','_','+','=','.','[',']','\\\\','/',';',':']\n",
    "F_data,S_data = utl.data_check(F_data,S_data,dic)\n",
    "Feature_eval,Scenario_eval = utl.data_check(Feature_eval,Scenario_eval,dic)\n",
    "Feature_list,Scenario_list = utl.data_check(Feature_list,Scenario_list,dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare model config and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref_data=pd.read_excel('./data/Feature2Scenario.xlsx').iloc[:,:].values\n",
    "# 可选模型：all-MiniLM-L6-v2、bert-base-chinese、ms-marco-MiniLM-L-6-v2、multi-qa-MiniLM-L6-cos-v1、paraphrase-MiniLM-L6-v2\n",
    "# 最终使用微调后模型，上述模型中性能最好的是bert-base-chinese\n",
    "\n",
    "model_name= 'bert-base-chinese'\n",
    "ct = config.config(finetuneFile='./finetune_model/',  #微调模型保存路径\n",
    "                    pretrainFile = './pretrain_model/', #预训练模型保存路径\n",
    "                    resultsFile = './Results/', #结果保存路径\n",
    "                    initial = False, #用于选取读取预训练模型还是微调模型，False为读取微调模型\n",
    "                    model_name = model_name, #选取模型名字，目前准备模型有如上注释\n",
    "                    max_expand = 1.5, #人工检验中用于扩大人工选择匹配文本对相似度的扩大倍数，该数据通过案例测试选定，可以根据后续数据调整\n",
    "                    ref_data = ref_data\n",
    "            )\n",
    "#print(ct.finetuneFile,ct.train_log)\n",
    "Bert_model = NLP_model.NLP_sim(ct) #模型初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算模型的相似性结果表，表的横排为当前输入所有feature，纵排为所有scenario，结果保存到config.ResultsFile对应文件夹中\n",
    "# 参数说明：\n",
    "## 第一个参数表示是否采用归一化计算；\n",
    "## 第二个参数为输入的Feature数据，可以单个可以多个；\n",
    "## 第三个参数为输入的所有Scenario；\n",
    "## 第四个为文件保存名字\n",
    "\n",
    "# sim为所有feature对所有scenario的相似度，为矩阵\n",
    "# 计算的相似度是cos对应的向量相似度，并非概率分布\n",
    "# order_results为所有feature对应sceanrio按照相似度进行排序的结果\n",
    "sim,embeddings_F,embeddings_S = Bert_model.Get_SimMatrix(True,F_data,list(set(S_data)),model_name+'_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在上述模型计算结果基础上，通过需求给定多路输出，计算筛选最终的scenario\n",
    "k1,k2,b = 1.5, 1.5, 0.75 # BM2.5模型筛选所需参数，取值参考：https://zhuanlan.zhihu.com/p/79202151\n",
    "order_scenario = Bert_model.Select_Scenario(F_data,list(set(S_data)),sim,k1,k2,b,model_name+'_orderedScenario.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 补充单个feature输入的计算结果，\n",
    "## 以'加速踏板防误踩 AMAP  (Anti-maloperation for Accelerator Pedal)'为例，可以替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个模块也可以单个结果输出，输入一个feature，输出该feature对应所有scenario的相似度\n",
    "test_feature = ['加速踏板防误踩 AMAP  (Anti-maloperation for Accelerator Pedal)']\n",
    "# sim为所有feature对所有scenario的相似度，为矩阵\n",
    "# order_results为所有feature对应sceanrio按照相似度进行排序的结果\n",
    "sim_single,embeddings_Fs,embeddings_Ss = Bert_model.Get_SimMatrix(True,test_feature,list(set(S_data)),model_name+'_single.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 由于业务部门提出“需要查看给定的feature与scenario对的相似度结果以进行检验”，以下代码为此专门编写\n",
    "## 输入一个feature，一个scenario，输出这一对的相似度。循环计算所有文本对的相似度，保存为csv以方便查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备测试文本数据\n",
    "F_data = ref_data2.copy()[:,0].tolist()\n",
    "S_data = ref_data2.copy()[:,1].tolist()\n",
    "# 文本数据计算embedding\n",
    "Fembedding = Bert_model._GetEmbedding_(F_data)\n",
    "Sembedding = Bert_model._GetEmbedding_(S_data)\n",
    "\n",
    "# 因为需要归一化计算，因此准备给定的标准scenario\n",
    "add_S = ref_data2.copy()[:,1].tolist()\n",
    "SS_data = list(set(pd.read_excel('./data/S_Data.xlsx').iloc[:,:].values.T[0].tolist()+add_S))\n",
    "# 计算标准scenario的embedding\n",
    "SSembedding = Bert_model._GetEmbedding_(SS_data)\n",
    "# 结果保存为results变量，后续存储为csv\n",
    "results = [['Scenario','Feature','sim number original','sim number softmax','sim number normalized']]\n",
    "# 计算给定的每个feature与scenario对的归一化相似度\n",
    "for s,f,em1,em2 in zip(S_data,F_data,Fembedding,Sembedding):\n",
    "     # 计算相似度 包含softmax\n",
    "     cos_scores = Bert_model._GetSim_cos_(em1,em2)\n",
    "     base_scores = Bert_model._GetSim_cos_(em1,SSembedding)\n",
    "     softmax_sim_base = np.exp(base_scores.numpy())/np.sum(np.exp(base_scores.numpy()))\n",
    "     softmax_sim = np.exp(cos_scores.numpy())/np.sum(np.exp(base_scores.numpy()))\n",
    "     # 计算归一化后的相似度\n",
    "     hc_sim = (softmax_sim-softmax_sim_base.min())/(softmax_sim_base.max()-softmax_sim_base.min())\n",
    "     tem=[s,f,\n",
    "          cos_scores[0],\n",
    "          softmax_sim[0],\n",
    "          hc_sim[0],\n",
    "          ]\n",
    "     results.append(tem)\n",
    "pd.DataFrame(results).to_csv('./Results/'+model_name+'_Final_results_2.csv',encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 132/132 [06:19<00:00,  2.88s/it]\n",
      "Iteration: 100%|██████████| 132/132 [06:10<00:00,  2.81s/it]\n",
      "Iteration: 100%|██████████| 132/132 [06:09<00:00,  2.80s/it]\n",
      "Iteration: 100%|██████████| 132/132 [06:07<00:00,  2.78s/it]\n",
      "Iteration: 100%|██████████| 132/132 [06:10<00:00,  2.81s/it]\n",
      "Iteration: 100%|██████████| 132/132 [06:11<00:00,  2.81s/it]\n",
      "Iteration: 100%|██████████| 132/132 [06:11<00:00,  2.81s/it]\n",
      "Iteration: 100%|██████████| 132/132 [06:08<00:00,  2.79s/it]\n",
      "Iteration: 100%|██████████| 132/132 [06:11<00:00,  2.81s/it]\n",
      "Iteration: 100%|██████████| 132/132 [06:08<00:00,  2.79s/it]\n",
      "Epoch: 100%|██████████| 10/10 [1:03:08<00:00, 378.85s/it]\n"
     ]
    }
   ],
   "source": [
    "train_loss,evaluator,eval = Bert_model.Fine_tune(Feature_list,Scenario_list,Feature_eval,Scenario_eval,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 应上一轮修改需求，检验模型对特殊字符的适应性，Test special vocab\n",
    "## 所有字符（包括特殊字符）数据给定在'./data/vocab.txt'中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检验特殊字符，并将检验结果保存到csv\n",
    "data = pd.read_table('./data/vocab.txt').values\n",
    "Vembedding = []\n",
    "for dt in data:\n",
    "    Vembedding.append(Bert_model._GetEmbedding_(dt[0]).tolist())\n",
    "pdv = pd.DataFrame(np.array(Vembedding).T)\n",
    "pdv.to_csv('./Results/vacab_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain示例\n",
    "### 由于补充收集了“相关数据”，而该补充数据由于缺少人工的“相似与否”标定，只能用于预训练，因此补充预训练\n",
    "### 由于预训练需要大量数据与计算资源，而目前收集的补充数据数量上无法达到这个标准，因此目前收集的数据并不能有效支持该过程取得完美效果，\n",
    "### 但该接口是可用的，故保留，以便后续数据足够时使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备预训练数据\n",
    "# 预训练数据包括两个部分，hugging face上获取数据，以及知乎专栏获取数据\n",
    "pre_train_data = pd.read_csv('./data/pretrain_traindata.csv').iloc[:,1].values.tolist()+pd.read_csv('./data/pretrain_traindata2.csv').iloc[:,1].values.tolist()\n",
    "pre_test_data = pd.read_csv('./data/pretrain_testdata.csv').iloc[:,1].values.tolist()+pd.read_csv('./data/pretrain_testdata2.csv').iloc[:,1].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除数据中空值\n",
    "tem_pre_train_data, tem_pre_test_data = [], []\n",
    "for item,k in zip(pre_train_data,range(len(pre_train_data))):\n",
    "    if type(item) == type(pre_train_data[0]):\n",
    "        tem_pre_train_data.append(item)\n",
    "\n",
    "for item,k in zip(pre_test_data,range(len(pre_test_data))):\n",
    "    if type(item) == type(pre_test_data[0]):\n",
    "        tem_pre_test_data.append(item)\n",
    "\n",
    "pre_train_data = tem_pre_train_data\n",
    "pre_test_data = tem_pre_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备预训练参数\n",
    "# 由于验收时间关系，这些参数仅使用已有代码的推荐参数，没有进行cross-validation，属于benchmark hyperparameter\n",
    "pre_ct = config.pretrain_config(per_device_train_batch_size=64,  #batch_size\n",
    "                    save_steps = 1, #Save model every given steps\n",
    "                    num_train_epochs = 1, #Number of epochs\n",
    "                    use_fp16 = False, #Set to True, if your GPU supports FP16 operations\n",
    "                    max_length = 512, #Max length for a text input\n",
    "                    do_whole_word_mask = True, #If set to true, whole words are masked\n",
    "                    mlm_prob = 0.15 #Probability that a word is replaced by a [MASK] token\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./pretrain_model/bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      " 50%|█████     | 1/2 [00:14<00:14, 14.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6962, 'learning_rate': 2.5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 1/2 [01:29<00:14, 14.21s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5047575235366821, 'eval_runtime': 75.6461, 'eval_samples_per_second': 15.017, 'eval_steps_per_second': 1.877, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:42<00:00, 58.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7433, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [02:58<00:00, 58.01s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4644050598144531, 'eval_runtime': 75.2966, 'eval_samples_per_second': 15.087, 'eval_steps_per_second': 1.886, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:59<00:00, 89.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 179.5064, 'train_samples_per_second': 0.557, 'train_steps_per_second': 0.011, 'train_loss': 1.7197410464286804, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# 由于目前预训练使用的TrainingArguments无法读取中文，所以需要输入保存模型的路径参数，并且该参数中不能包含中文（不能保存在中文路径）\n",
    "save_path = 'C:/testmodel/'\n",
    "Bert_model.Pre_train(pre_train_data[:100],pre_test_data,pre_ct,save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
